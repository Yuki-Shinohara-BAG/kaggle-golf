{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44849d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " System Information\n",
      "============================================================\n",
      "üêç Python version: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from typing import Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_header(title: str):\n",
    "    \"\"\"Print formatted header\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" {title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "def print_result(library: str, available: bool, details: str = \"\"):\n",
    "    \"\"\"Print formatted result\"\"\"\n",
    "    status = \"‚úÖ Available\" if available else \"‚ùå Not Available\"\n",
    "    print(f\"{library:<15}: {status}\")\n",
    "    if details:\n",
    "        print(f\"                {details}\")\n",
    "\n",
    "def print_summary(name: str, result: Dict[str, Any]):\n",
    "    \"\"\"Print summary for each library\"\"\"\n",
    "    print_header(f\"{name} GPU Check\")\n",
    "    if result['version']:\n",
    "        print(f\"üì¶ Version: {result['version']}\")\n",
    "    else:\n",
    "        print(f\"üì¶ Not installed\")\n",
    "    \n",
    "    print_result(name, result['available'], result['details'])\n",
    "    \n",
    "    if result['available']:\n",
    "        print(\"üéâ Ready for GPU acceleration!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  GPU acceleration not available\")\n",
    "\n",
    "# System Information\n",
    "print_header(\"System Information\")\n",
    "print(f\"üêç Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a877ce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " PyTorch GPU Check\n",
      "============================================================\n",
      "üì¶ PyTorch version: 2.7.0+cu126\n",
      "üîç CUDA available: True\n",
      "üñ•Ô∏è  Device count: 1\n",
      "üéØ Current device: 0\n",
      "üíæ Device name: NVIDIA GeForce RTX 4090\n",
      "üîß CUDA version: 12.6\n",
      "üèóÔ∏è  cuDNN version: 90501\n",
      "\n",
      "üß™ Running GPU test...\n",
      "   Input tensor: [1. 2. 3. 4.]\n",
      "   Output tensor: [2. 4. 6. 8.]\n",
      "   Matrix multiplication result: 60.0\n",
      "‚úÖ GPU computation successful!\n"
     ]
    }
   ],
   "source": [
    "print_header(\"PyTorch GPU Check\")\n",
    "\n",
    "def check_pytorch():\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
    "        \n",
    "        # CUDA availability\n",
    "        cuda_available = torch.cuda.is_available()\n",
    "        print(f\"üîç CUDA available: {cuda_available}\")\n",
    "        \n",
    "        if cuda_available:\n",
    "            device_count = torch.cuda.device_count()\n",
    "            current_device = torch.cuda.current_device()\n",
    "            device_name = torch.cuda.get_device_name(0)\n",
    "            \n",
    "            print(f\"üñ•Ô∏è  Device count: {device_count}\")\n",
    "            print(f\"üéØ Current device: {current_device}\")\n",
    "            print(f\"üíæ Device name: {device_name}\")\n",
    "            \n",
    "            # CUDA version info\n",
    "            print(f\"üîß CUDA version: {torch.version.cuda}\")\n",
    "            print(f\"üèóÔ∏è  cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "            \n",
    "            # Simple GPU test\n",
    "            try:\n",
    "                print(\"\\nüß™ Running GPU test...\")\n",
    "                x = torch.tensor([1.0, 2.0, 3.0, 4.0]).cuda()\n",
    "                y = x * 2\n",
    "                z = torch.matmul(x.unsqueeze(0), y.unsqueeze(1))\n",
    "                \n",
    "                print(f\"   Input tensor: {x.cpu().numpy()}\")\n",
    "                print(f\"   Output tensor: {y.cpu().numpy()}\")\n",
    "                print(f\"   Matrix multiplication result: {z.cpu().item()}\")\n",
    "                print(\"‚úÖ GPU computation successful!\")\n",
    "                \n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå GPU test failed: {str(e)}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"‚ùå CUDA not available\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ùå PyTorch not installed\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run PyTorch check\n",
    "pytorch_result = check_pytorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9b2903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " TensorFlow GPU Check\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 03:03:19.401306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748541799.410665   12422 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748541799.413501   12422 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748541799.420881   12422 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748541799.420888   12422 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748541799.420889   12422 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748541799.420890   12422 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ TensorFlow version: 2.19.0\n",
      "üîç GPUs found: 1\n",
      "üñ•Ô∏è  GPU 0: /physical_device:GPU:0\n",
      "üîß Memory growth enabled\n",
      "\n",
      "üß™ Running GPU test...\n",
      "   Matrix A:\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "   Matrix B:\n",
      "[[2. 0.]\n",
      " [0. 2.]]\n",
      "   A √ó B:\n",
      "[[2. 4.]\n",
      " [6. 8.]]\n",
      "‚úÖ GPU computation successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748541800.772229   12422 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22117 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "print_header(\"TensorFlow GPU Check\")\n",
    "\n",
    "def check_tensorflow():\n",
    "    try:\n",
    "        import os\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress warnings\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        print(f\"üì¶ TensorFlow version: {tf.__version__}\")\n",
    "        \n",
    "        # GPU device listing\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        print(f\"üîç GPUs found: {len(gpus)}\")\n",
    "        \n",
    "        if len(gpus) > 0:\n",
    "            for i, gpu in enumerate(gpus):\n",
    "                print(f\"üñ•Ô∏è  GPU {i}: {gpu.name}\")\n",
    "                \n",
    "            # Memory growth setting (optional)\n",
    "            try:\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                print(\"üîß Memory growth enabled\")\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è  Could not enable memory growth\")\n",
    "            \n",
    "            # Simple GPU test\n",
    "            try:\n",
    "                print(\"\\nüß™ Running GPU test...\")\n",
    "                with tf.device('/GPU:0'):\n",
    "                    a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "                    b = tf.constant([[2.0, 0.0], [0.0, 2.0]])\n",
    "                    c = tf.matmul(a, b)\n",
    "                    \n",
    "                print(f\"   Matrix A:\\n{a.numpy()}\")\n",
    "                print(f\"   Matrix B:\\n{b.numpy()}\")\n",
    "                print(f\"   A √ó B:\\n{c.numpy()}\")\n",
    "                print(\"‚úÖ GPU computation successful!\")\n",
    "                \n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå GPU test failed: {str(e)}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"‚ùå No GPU devices found\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ùå TensorFlow not installed\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run TensorFlow check\n",
    "tensorflow_result = check_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967ea0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " CatBoost GPU Check\n",
      "============================================================\n",
      "üì¶ CatBoost version: 1.2.8\n",
      "üß™ Creating test dataset...\n",
      "   Dataset shape: (100, 5)\n",
      "   Target shape: (100,)\n",
      "\n",
      "üß™ Testing GPU training...\n",
      "   Sample predictions: [0 0 0 1 0]\n",
      "   Sample probabilities shape: (5, 2)\n",
      "‚úÖ GPU training successful!\n"
     ]
    }
   ],
   "source": [
    "print_header(\"CatBoost GPU Check\")\n",
    "\n",
    "def check_catboost():\n",
    "    try:\n",
    "        import catboost\n",
    "        from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "        import numpy as np\n",
    "        \n",
    "        print(f\"üì¶ CatBoost version: {catboost.__version__}\")\n",
    "        \n",
    "        # Test data\n",
    "        print(\"üß™ Creating test dataset...\")\n",
    "        X = np.random.randn(100, 5)\n",
    "        y = np.random.randint(0, 2, 100)\n",
    "        \n",
    "        print(f\"   Dataset shape: {X.shape}\")\n",
    "        print(f\"   Target shape: {y.shape}\")\n",
    "        \n",
    "        # GPU test\n",
    "        try:\n",
    "            print(\"\\nüß™ Testing GPU training...\")\n",
    "            model = CatBoostClassifier(\n",
    "                iterations=10,\n",
    "                task_type=\"GPU\",\n",
    "                devices='0',\n",
    "                verbose=False,\n",
    "                random_seed=42\n",
    "            )\n",
    "            \n",
    "            model.fit(X, y, verbose=False)\n",
    "            predictions = model.predict(X[:5])\n",
    "            probabilities = model.predict_proba(X[:5])\n",
    "            \n",
    "            print(f\"   Sample predictions: {predictions}\")\n",
    "            print(f\"   Sample probabilities shape: {probabilities.shape}\")\n",
    "            print(\"‚úÖ GPU training successful!\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as gpu_error:\n",
    "            print(f\"‚ùå GPU training failed: {str(gpu_error)}\")\n",
    "            \n",
    "            # Try CPU fallback\n",
    "            try:\n",
    "                print(\"üîÑ Trying CPU training...\")\n",
    "                model_cpu = CatBoostClassifier(\n",
    "                    iterations=10,\n",
    "                    verbose=False,\n",
    "                    random_seed=42\n",
    "                )\n",
    "                model_cpu.fit(X, y, verbose=False)\n",
    "                print(\"‚úÖ CPU training works\")\n",
    "                print(\"‚ö†Ô∏è  But GPU is not available\")\n",
    "                return False\n",
    "            except Exception as cpu_error:\n",
    "                print(f\"‚ùå CPU training also failed: {str(cpu_error)}\")\n",
    "                return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ùå CatBoost not installed\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run CatBoost check\n",
    "catboost_result = check_catboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628e63ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " XGBoost GPU Check\n",
      "============================================================\n",
      "üì¶ XGBoost version: 3.0.2\n",
      "üß™ Creating test dataset...\n",
      "   Training set: (800, 10)\n",
      "   Test set: (200, 10)\n",
      "\n",
      "üß™ Testing GPU training...\n",
      "   Sample predictions: [0.08655342 0.9932041  0.32216457 0.89159507 0.00507787]\n",
      "‚úÖ GPU training successful!\n"
     ]
    }
   ],
   "source": [
    "print_header(\"XGBoost GPU Check\")\n",
    "\n",
    "def check_xgboost():\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        import numpy as np\n",
    "        from sklearn.datasets import make_classification\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        print(f\"üì¶ XGBoost version: {xgb.__version__}\")\n",
    "        \n",
    "        # Test data\n",
    "        print(\"üß™ Creating test dataset...\")\n",
    "        X, y = make_classification(\n",
    "            n_samples=1000, \n",
    "            n_features=10, \n",
    "            n_classes=2, \n",
    "            random_state=42\n",
    "        )\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"   Training set: {X_train.shape}\")\n",
    "        print(f\"   Test set: {X_test.shape}\")\n",
    "        \n",
    "        # Create DMatrix\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "        \n",
    "        # GPU test\n",
    "        try:\n",
    "            print(\"\\nüß™ Testing GPU training...\")\n",
    "            params_gpu = {\n",
    "                'tree_method': 'gpu_hist',\n",
    "                'gpu_id': 0,\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'logloss',\n",
    "                'verbosity': 0,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            model_gpu = xgb.train(\n",
    "                params_gpu,\n",
    "                dtrain,\n",
    "                num_boost_round=20,\n",
    "                evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            \n",
    "            # Make predictions\n",
    "            preds = model_gpu.predict(dtest)\n",
    "            print(f\"   Sample predictions: {preds[:5]}\")\n",
    "            print(\"‚úÖ GPU training successful!\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as gpu_error:\n",
    "            print(f\"‚ùå GPU training failed: {str(gpu_error)}\")\n",
    "            \n",
    "            # Try CPU fallback\n",
    "            try:\n",
    "                print(\"üîÑ Trying CPU training...\")\n",
    "                params_cpu = {\n",
    "                    'tree_method': 'hist',\n",
    "                    'objective': 'binary:logistic',\n",
    "                    'eval_metric': 'logloss',\n",
    "                    'verbosity': 0,\n",
    "                    'random_state': 42\n",
    "                }\n",
    "                \n",
    "                model_cpu = xgb.train(\n",
    "                    params_cpu,\n",
    "                    dtrain,\n",
    "                    num_boost_round=20,\n",
    "                    verbose_eval=False\n",
    "                )\n",
    "                print(\"‚úÖ CPU training works\")\n",
    "                print(\"‚ö†Ô∏è  But GPU is not available\")\n",
    "                return False\n",
    "            except Exception as cpu_error:\n",
    "                print(f\"‚ùå CPU training also failed: {str(cpu_error)}\")\n",
    "                return False\n",
    "                \n",
    "    except ImportError:\n",
    "        print(\"‚ùå XGBoost not installed\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run XGBoost check\n",
    "xgboost_result = check_xgboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "738f147c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " üéØ Final Summary & Recommendations\n",
      "============================================================\n",
      "üìä GPU Status Overview:\n",
      "   Total libraries checked: 4\n",
      "   GPU-enabled libraries: 4\n",
      "   GPU success rate: 100.0%\n",
      "\n",
      "üìã Individual Results:\n",
      "   üü¢ PyTorch: GPU Ready\n",
      "   üü¢ TensorFlow: GPU Ready\n",
      "   üü¢ CatBoost: GPU Ready\n",
      "   üü¢ XGBoost: GPU Ready\n",
      "\n",
      "üí° Recommendations:\n",
      "   üéâ Perfect! All libraries can use GPU acceleration.\n",
      "   üöÄ You're ready for high-performance ML/DL workloads!\n",
      "\n",
      "üîó Useful Commands:\n",
      "   nvidia-smi                    # Check GPU status\n",
      "   docker run --gpus all ...     # Enable GPU in Docker\n",
      "   uv add torch --index-url https://download.pytorch.org/whl/cu118  # GPU PyTorch\n"
     ]
    }
   ],
   "source": [
    "print_header(\"üéØ Final Summary & Recommendations\")\n",
    "\n",
    "# Collect results (run this after all previous cells)\n",
    "results = {\n",
    "    'PyTorch': pytorch_result if 'pytorch_result' in locals() else False,\n",
    "    'TensorFlow': tensorflow_result if 'tensorflow_result' in locals() else False,\n",
    "    'CatBoost': catboost_result if 'catboost_result' in locals() else False,\n",
    "    'XGBoost': xgboost_result if 'xgboost_result' in locals() else False\n",
    "}\n",
    "\n",
    "# Count GPU-enabled libraries\n",
    "gpu_enabled = sum(results.values())\n",
    "total_checked = len(results)\n",
    "\n",
    "print(f\"üìä GPU Status Overview:\")\n",
    "print(f\"   Total libraries checked: {total_checked}\")\n",
    "print(f\"   GPU-enabled libraries: {gpu_enabled}\")\n",
    "print(f\"   GPU success rate: {gpu_enabled/total_checked*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüìã Individual Results:\")\n",
    "for lib, status in results.items():\n",
    "    emoji = \"üü¢\" if status else \"üî¥\"\n",
    "    print(f\"   {emoji} {lib}: {'GPU Ready' if status else 'GPU Not Available'}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "if gpu_enabled == total_checked:\n",
    "    print(\"   üéâ Perfect! All libraries can use GPU acceleration.\")\n",
    "    print(\"   üöÄ You're ready for high-performance ML/DL workloads!\")\n",
    "elif gpu_enabled > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  {total_checked - gpu_enabled} libraries cannot use GPU.\")\n",
    "    print(\"   üîß Check CUDA/driver compatibility for missing libraries.\")\n",
    "else:\n",
    "    print(\"   ‚ùå No GPU acceleration available.\")\n",
    "    print(\"   üîß Check your NVIDIA drivers, CUDA installation, and Docker GPU setup.\")\n",
    "\n",
    "print(f\"\\nüîó Useful Commands:\")\n",
    "print(\"   nvidia-smi                    # Check GPU status\")\n",
    "print(\"   docker run --gpus all ...     # Enable GPU in Docker\")\n",
    "print(\"   uv add torch --index-url https://download.pytorch.org/whl/cu118  # GPU PyTorch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
